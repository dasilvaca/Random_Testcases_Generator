{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Explanation and Documentation [The Input Generator Inator]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authors:\n",
    "* [Ceballos Aguilar Laura Valentina](https://github.com/lceballosa1) \n",
    "* [DÃ­az Medina Cesar Esteban](https://github.com/Cesardiaz18)\n",
    "* [Silva Capera Daniel Santiago](https://github.com/dasilvaca)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "### Motivation and Justification\n",
    "\n",
    "Nowadays, in some ocassions, with problems of different types on competitive programming and in academic environments, when some test cases are available, or a kind of specific testcase type is given, instead of solving the problem in the way it must have to, sometimes it's hardcoded. This could happen more often when all the testcases are shown. Sometimes, there's a penalty when is done, but, the real problem is that the problem is not really solved, and sometimes this is not even noticed. For sure, the real purpose of the problem is to learn, not just give some expected outputs. That's why this prjoject implementation is important.\n",
    "\n",
    "### Related works and Background\n",
    "\n",
    "In not implemented solution, like in software production, there are used some random input generators, but are not really helpful, because, there are not compared to actual expected outputs given, but are not generated randomly with some frequence, also with some few cases at the output, some that the tester thinks could be border cases.In this environment it's not expected that the coder learns, the only need here is to avoid some possible production bugs.\n",
    "\n",
    "This is an [example and explanation](https://damorimrg.github.io/practical_testing_book/testgeneration/random.html) of randomized testing nowadays capabilities.\n",
    "\n",
    "Objective\n",
    "\n",
    "This project born in order to avoid *hardcoding* the data in the code when someone (like students or program competitors) submit solutions in contests and related evaluative code tasks. The idea is to request to the problem submitter to provide the expected input data format and possible values, in order to generate random expected output for each code submission using a sample code that works perfectly fine to solve the problem. In this way, the code evaluator can be sure that the code is not *hardcoded* and the code solution submitter must have to avoid this practice.\n",
    "\n",
    "In this order, the project must have to do the following:\n",
    "\n",
    "- Read the expected output data format and possible values from a file. With this information, do some random expected outputs with the veredict that the correct solution provides.\n",
    "- Compare with the code solution submitter output and provide the veredict.\n",
    "- Erradicate hardcoded submissions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
